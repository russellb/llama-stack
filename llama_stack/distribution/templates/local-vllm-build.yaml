name: local-vllm
distribution_spec:
  description: Like local, but use vLLM for running LLM inference
  docker_image: docker.io/nvidia/cuda:12.6.1-cudnn-runtime-ubuntu24.04
  providers:
    inference: vllm
    memory: meta-reference
    safety: meta-reference
    agents: meta-reference
    telemetry: meta-reference
image_type: docker
